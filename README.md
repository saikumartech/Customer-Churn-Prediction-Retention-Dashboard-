📊 Customer Churn Prediction & Retention Dashboard

An end-to-end data science project that predicts customer churn and visualizes actionable retention insights using machine learning and data analytics.

This notebook walks through the full pipeline: from data loading and exploration, through preprocessing, to model development and dashboard creation.

---

📌 Project Objective

Customer churn is a critical metric for businesses that rely on long-term user engagement. This project aims to:

- Predict whether a customer is likely to churn
- Identify key drivers behind churn behavior
- Provide insights that can help businesses improve retention
- Present results through clean visuals and (optionally) an interactive dashboard

---

✅ Completed (So Far)

- 🔹 Project setup and documentation
- 🔹 Dataset download using Kaggle API
- 🔹 Exploratory Data Analysis (EDA) with visualizations
- 🔹 Data cleaning and preprocessing
  - Null handling
  - Feature encoding
  - Feature scaling
  - Train-test split

---

🔜 Next Steps (Upcoming)

- 🤖 Train baseline ML models (Logistic Regression, Random Forest)
- 📊 Evaluate model performance (Accuracy, F1, ROC-AUC)
- 📈 Interpret feature importance
- 📊 Build and share interactive dashboard (Streamlit / Power BI)
- 📝 Final project documentation + insights summary

---

🧠 Technologies Used

- **Python**, **Pandas**, **NumPy** – Data processing
- **Seaborn**, **Matplotlib** – Visualizations
- **Scikit-learn** – Machine learning
- **Google Colab** – Development environment
- **Kaggle API** – Dataset access
- **GitHub** – Project version control

---

📁 Project Structure


---

## 🧾 Dataset

- **Source**: [Kaggle – Telco Customer Churn](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
- **Description**: Contains telecom customer data such as tenure, payment type, service type, and churn status

---

## 🙋 Author

**Sai Kumar**  
Aspiring Data Scientist | Python | Machine Learning | Power BI  
[LinkedIn](https://www.linkedin.com/in/sai-kumar-akasapu-219165167/)

---

📌 *This project is a work in progress and updated daily as part of a public data science portfolio.*

